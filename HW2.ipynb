{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Markov\n",
    "* 实现下图的环境，需要实现环境中的动态转移函数。\n",
    "* 实现一个 agent, 策略是随机的，通过仿真的方式，用回报值的经验平均去估计每个状态的值函数。验证仿真的结果和课件中计算的结果。(分别仿真  γ  = 0.5, 1)\n",
    "* 强化学习中寻找最优策略的方法有很多种，其中全局遍历是朴素的解法，由于对于MDPs总存在最优的确定性策略，通过全局遍历所有确定性策略，并比较策略即可求出最优策略。在该MDPs中只有四个状态可以决策，每个状态只有两个可行的动作，所以总共有16个确定性策略，使用算法遍历所有策略，并输出最优策略。(分别考虑  γ  = 0.5, 1)\n",
    "\n",
    "![](http://ww2.sinaimg.cn/large/006tNc79ly1g4xyd7ugfxj30ey0e6755.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('s5', 10, True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from code2_markov import Env, Agent\n",
    "\n",
    "env = Env()\n",
    "env.step('s4', 'review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'phone'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = Agent()\n",
    "agent.random_policy('s1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_eval(s, gamma, max_step=100, N=10000):\n",
    "    gs = []\n",
    "    for _ in range(N):\n",
    "        g = 0\n",
    "        curr_s = s\n",
    "        curr_gamma = 1\n",
    "        for step in range(max_step):\n",
    "            a = agent.policy(curr_s)\n",
    "            curr_s, r, term = env.step(curr_s, a)\n",
    "            g += curr_gamma * r\n",
    "            curr_gamma *= gamma\n",
    "            if term:\n",
    "                break\n",
    "        gs.append(g)\n",
    "    return np.average(gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(s1): -4.6086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(s2): -3.5991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(s3): 0.4792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(s4): 2.8575\n"
     ]
    }
   ],
   "source": [
    "for s in ['s1', 's2', 's3', 's4']:\n",
    "    print('V({}): {}'.format(s, simulate_eval(s, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以和课件中的结果对比，注意到增加max_step和N的值是可以减小误差的。\n",
    "> 课件中的结果是通过在给定策略的情况下，将MDP转化成MRP，然后用矩阵求解的\n",
    "![](http://ww3.sinaimg.cn/large/006tNc79ly1g4xz0d5cnij30f80e6t9k.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(s1): -1.3025859339088348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(s2): -1.9081474046711446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(s3): -0.3386011561062558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(s4): 2.6385232126889866\n"
     ]
    }
   ],
   "source": [
    "for s in ['s1', 's2', 's3', 's4']:\n",
    "    print('V({}): {}'.format(s, simulate_eval(s, 0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们通过遍历所有的确定性策略，来寻找最优策略\n",
    "\n",
    "- 首先需要确定有哪些确定性策略\n",
    "- 然后选择哪个确定性策略最好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s1': 'phone', 's2': 'phone', 's3': 'study', 's4': 'review'}\n{'s1': 'phone', 's2': 'phone', 's3': 'study', 's4': 'noreview'}\n{'s1': 'phone', 's2': 'phone', 's3': 'sleep', 's4': 'review'}\n{'s1': 'phone', 's2': 'phone', 's3': 'sleep', 's4': 'noreview'}\n{'s1': 'phone', 's2': 'study', 's3': 'study', 's4': 'review'}\n{'s1': 'phone', 's2': 'study', 's3': 'study', 's4': 'noreview'}\n{'s1': 'phone', 's2': 'study', 's3': 'sleep', 's4': 'review'}\n{'s1': 'phone', 's2': 'study', 's3': 'sleep', 's4': 'noreview'}\n{'s1': 'quit', 's2': 'phone', 's3': 'study', 's4': 'review'}\n{'s1': 'quit', 's2': 'phone', 's3': 'study', 's4': 'noreview'}\n{'s1': 'quit', 's2': 'phone', 's3': 'sleep', 's4': 'review'}\n{'s1': 'quit', 's2': 'phone', 's3': 'sleep', 's4': 'noreview'}\n{'s1': 'quit', 's2': 'study', 's3': 'study', 's4': 'review'}\n{'s1': 'quit', 's2': 'study', 's3': 'study', 's4': 'noreview'}\n{'s1': 'quit', 's2': 'study', 's3': 'sleep', 's4': 'review'}\n{'s1': 'quit', 's2': 'study', 's3': 'sleep', 's4': 'noreview'}\n"
     ]
    }
   ],
   "source": [
    "deterministic_policies = [\n",
    "    dict(s1=a1, s2=a2, s3=a3, s4=a4)\n",
    "    for a1 in agent.available_actions['s1']\n",
    "    for a2 in agent.available_actions['s2']\n",
    "    for a3 in agent.available_actions['s3']\n",
    "    for a4 in agent.available_actions['s4']\n",
    "]\n",
    "\n",
    "for policy in deterministic_policies:\n",
    "    print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1: phone\ns2: phone\ns3: study\ns4: review\n"
     ]
    }
   ],
   "source": [
    "def wrap_policy(deterministic_policies, index):\n",
    "    policy_dict = deterministic_policies[index]\n",
    "    def policy(s):\n",
    "        return policy_dict[s]\n",
    "    return policy\n",
    "\n",
    "agent.policy = wrap_policy(deterministic_policies, 0)\n",
    "for s in ['s1', 's2', 's3', 's4']:\n",
    "    print('{}: {}'.format(s, agent.policy(s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们接下来统计这些策略所能实现的值函数\n",
    "\n",
    "- 这里只计算了 γ =0.5的结果， γ =1同理\n",
    "- 为了降低计算量，max_step和N选择较小的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulate 0th deterministic policy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulate 1th deterministic policy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulate 2th deterministic policy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulate 3th deterministic policy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulate 4th deterministic policy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulate 5th deterministic policy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulate 6th deterministic policy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulate 7th deterministic policy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulate 8th deterministic policy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulate 9th deterministic policy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulate 10th deterministic policy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulate 11th deterministic policy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulate 12th deterministic policy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulate 13th deterministic policy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulate 14th deterministic policy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulate 15th deterministic policy\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.5\n",
    "values = []\n",
    "for index in range(len(deterministic_policies)):\n",
    "    print('Simulate {}th deterministic policy'.format(index))\n",
    "    value_for_policy = []\n",
    "    agent.policy = wrap_policy(deterministic_policies, index)\n",
    "    for s in ['s1', 's2', 's3', 's4']:\n",
    "        value_for_policy.append(simulate_eval(s, gamma, max_step=100, N=1000))\n",
    "    values.append(value_for_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单地，我们比较哪个策略所能实现的值函数最大，即能得到最优策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy\n{'s1': 'quit', 's2': 'study', 's3': 'study', 's4': 'review'}\n"
     ]
    }
   ],
   "source": [
    "max_sum_value = -1000000\n",
    "max_index = -1\n",
    "for i, value in enumerate(values):\n",
    "    sum_value = sum(value)\n",
    "    if sum_value > max_sum_value:\n",
    "        max_sum_value = sum_value\n",
    "        max_index = i\n",
    "print('Optimal policy')\n",
    "print(deterministic_policies[max_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
